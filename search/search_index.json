{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"What is observability\u00b6","text":""},{"location":"#what-it-is","title":"What it is\u00b6","text":"<p>Observability is the capability to continuously generate and discover actionable insights based on signals from the system under observation. In other words, observability allows users to understand a system\u2019s state from its external output and take (corrective) action.</p>"},{"location":"#problem-it-addresses","title":"Problem it addresses\u00b6","text":"<p>Computer systems are measured by observing low-level signals such as CPU time, memory, disk space, and higher-level and business signals, including API response times, errors, transactions per second, etc.</p> <p>The observability of a system has a significant impact on its operating and development costs. Observable systems yield meaningful, actionable data to their operators, allowing them to achieve favorable outcomes (faster incident response, increased developer productivity) and less toil and downtime.</p>"},{"location":"#how-it-helps","title":"How it helps\u00b6","text":"<p>Understanding that more information does not necessarily translate into a more observable system is pivotal. In fact, sometimes, the amount of information generated by a system can make it harder to identify valuable health signals from the noise generated by the application. Observability requires the right data at the right time for the right consumer (human or piece of software) to make the right decisions.</p>"},{"location":"#what-you-will-find-here","title":"What you will find here\u00b6","text":"<p>This site contains our best practices for observability: what do to, what not to do, and a collection of recipes on how to do them. Most of the content here is vendor agnostic and represents what any good observability solution will provide.</p> <p>It is important that you consider observability as a solution though and not a product. Observability comes from your practices, and is integral to strong development and DevOps leadership. A well-observed application is one that places observability as a principal of operations, similar to how security must be at the forefront of how you organize a project. Attempting to \u201cbolt-on\u201d observability after the fact is an anti-pattern and meets with less success.</p> <p>This site is organized into four categories:</p> <ol> <li>Best practices by solution, such as for dashboarding, application performance monitoring, or containers</li> <li>Best practices for the use of different data types, such as for logs or traces</li> <li>Best practices for specific AWS tools (though these are largely fungible to other vendor products as well)</li> <li>Curated recipes for observability with AWS</li> </ol> <p>Success</p> <p>This site is based on real world use cases that AWS and our customers have solved for. Observability is at the heart of modern application development, and a critical consideration when operating distributed systems, such as microservices, or complex applications with many external integrations. We consider it to be a leading indicator of a healthy workload, and we are pleased to share our experiences with you here!</p> <p>Hover me</p>"},{"location":"contributors/","title":"Contributors\u00b6","text":"<p>The content on this site is maintained by AWS open source observability service team members and other volunteers from across the organization. Our goal is to improve the discovery of relevant best practices on how to set up and use AWS services and open source projects in the observability space.</p> <p>Recipes and content contributions in general so far are from the following people:</p> Authors Authors Authors Authors Alolita Sharma Aly Shah Imtiaz Helen Ashton Elamaran Shanmugam Dinesh Boddula Imaya Kumar Jagannathan Dieter Adant Eric Hsueh Jason Derrett Kevin Lewin Mahesh Biradar Michael Hausenblas Munish Dabra Rich McDonough Rob Sable Rodrigue Koffi Sheetal Joshi Tomasz Wrzonski Tyler Lynch Vijayan Sarathy Vikram Venkataraman Yiming Peng Arun Chandapillai Alex Livingstone Kiran Prakash Bobby Hallahan Toshal Dudhwala Franklin Aguinaldo Nirmal Mehta Lucas Vieira Souza da Silva William Armiros Abhi Khanna <p>Note that all recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution.</p>"},{"location":"guides/","title":"Best practices overview\u00b6","text":"<p>Observability is a broad topic with a mature landscape of tools. Not every tool is right for every solution though! To help you navigate through your observability requirements, configuration, and final deployment, we have summarized five key best practices that will inform your decision making process on your Observability strategy.</p>"},{"location":"guides/#monitor-what-matters","title":"Monitor what matters\u00b6","text":"<p>The most important consideration with observability is not your servers, network, applications, or customers. It is what matters to you, your business, your project, or your users.</p> <p>Start first with what your success criteria are. For example, if you run an e-commerce application, your measures of success may be number of purchases made over the past hour. If you run a non-profit, then it may be donations vs. your target for the month. A payment processor may watch for transaction processing time, whereas universities would want to measure student attendance.</p> <p>Tip</p> <p>Success metrics are different for everyone! We may use an e-commerce application as an example here, but your projects can have a very different measurement. Regardless, the advice remains the same: know what good looks like and measure for it.</p> <p>Regardless of your application, you must start with identifying your key metrics. Then work backwards1 from that to see what impacts it from an application or infrastructure perspective. For example, if high CPU on your web servers endangers customer satisfaction, and in-turn your sales, then monitoring CPU utilization is important!</p>"},{"location":"guides/#know-your-objectives-and-measure-them","title":"Know your objectives, and measure them","text":"<p>Having identified your important top-level KPIs, your next job is to have an automated way to track and measure them. A critical success factor is doing so in the same system that watches your workload's operations. For our e-commerce workload example this may mean:</p> <ul> <li>Publishing sales data into a time series</li> <li>Tracking user registrations in this same system</li> <li>Measure how long customers stay on web pages, and (again) push this data to a time series</li> </ul> <p>Most customers have this data already, though not necessarily in the right places from an observability perspective. Sales data can typically be found in relational databases or business intelligence reporting systems, along with user registrations. And data from visit duration can be extracted from logs or from Real User Monitoring.</p> <p>Regardless of your metric data's original location or format, it must be maintained as a time series. Every key metric that matters most to you, whether it is business, personal, academic, or for any other purpose, must be in a time series format for you to correlate it with other observability data (sometimes known as signals or telemetry).</p> <p> Figure 1: example of a time series</p>"},{"location":"guides/#context-propagation-and-tool-selection","title":"Context propagation and tool selection\u00b6","text":"<p>Tool selection is important and has a profound difference in how you operate and remediate problems. But worse than choosing a sub-optimal tool is tooling for all basic signal types. For example, collecting basic logs from a workload, but missing transaction traces, leaves you with a gap. The result is an incohesive view of your entire application experiece. All modern approaches to observability depend on \"connecting the dots\" with application traces.</p> <p>A complete picture of your health and operations requires tools that collect logs, metrics, and traces, and then performs correlation, analysis, anomaly detection, dashboarding, alarms and more.</p> <p>Info</p> <p>Some observability solutions may not contain all of the above but are intended to augment, extend, or give added value to existing systems. In all cases, tool interoperability and extensibility is an important consideration when beginning an observability project.</p>"},{"location":"guides/#every-workload-is-different-but-common-tools-make-for-a-faster-results","title":"Every workload is different, but common tools make for a faster results\u00b6","text":"<p>Using a common set of tools across every workload has add benefits such as reducing operational friction and training, and generally you should strive for a reduced number of tools or vendors. Doing so lets you rapidly deploy existing observability solutions to new environments or workloads, and with faster time-to-resolution when things go wrong.</p> <p>Your tools should be broad enough to observe every tier of your workload: basic infrastructure, applications, web sites, and everything in between. In places where a single tool is not possible, the best practice is to use those that have an open standard, are open source, and therefore have the broadest cross-platform integration possibilities.</p>"},{"location":"guides/#integrate-with-existing-tools-and-processes","title":"Integrate with existing tools and processes\u00b6","text":"<p>Don't reinvent the wheel! \"Round\" is a great shape already, and we should always be building collaborative and open systems, not data silos.</p> <ul> <li>Integrate with existing identity providers (e.g. Active Directory, SAML based IdPs).</li> <li>If you have existing IT trouble tracking system (e.g. JIRA, ServiceNow) then integrate with it to quickly manage problems as they arise.</li> <li>Use existing workload management and escalation tools (e.g. PagerDuty, OpsGenie) if you already have them!</li> <li>Infrastructure as code tools such as Anisble, SaltStack, CloudFormation, TerraForm, and CDK are all great tools. Use them to manage your observability as well as everything else, and build your observability solution with the same infrastructure as code tools you already use today (see include observability from day one).</li> </ul>"},{"location":"guides/#use-automation-and-machine-learning","title":"Use automation and machine learning\u00b6","text":"<p>Computers are good at finding patterns, and at finding when data does not follow a pattern! If you have hundreds, thousands, or even millions of datapoints to monitor, then it would impossible to understand healthy thresholds for every single one of them. But many observability solutions have anomaly detection and machine learning capabilities that manage the undifferentiated heavy lifting of baselining your data.</p> <p>We refer to this as \"knowing what good looks like\". If you have load-tested your workload thoroughly then you may know these healthy performance metrics already, but for a complex distributed application it can be unwieldy to create baselines for every metric. This is where anomaly detection, automation, and machine learning are invaluable.</p> <p>Leverage tools that manage the baselining and alerting of applications health on your behalf, thereby letting you focus on your goals, and monitor what matters.</p>"},{"location":"guides/#collect-telemetry-from-all-tiers-of-your-workload","title":"Collect telemetry from all tiers of your workload\u00b6","text":"<p>Your applications do not exist in isolation, and interactions with your network infrastructure, cloud providers, internet service providers, SasS partners, and other components both within and outside your control can all impact your outcomes. It is important that you have a holistic view of your entire workload.</p>"},{"location":"guides/#focus-on-integrations","title":"Focus on integrations\u00b6","text":"<p>If you have to pick one area to instrument, it will undoubtedly be your integrations between components. This is where the power of observability is most evident. As a rule, every time one component or service calls another, that call must have at least these data points measured:</p> <ol> <li>The duration of the request and response</li> <li>The status of the response</li> </ol> <p>And to create the cohesive, holistic view that observability requires, a single unique identier for the entire request chain must be included in the signals collected.</p>"},{"location":"guides/#dont-forget-about-the-end-user-experience","title":"Don't forget about the end-user experience\u00b6","text":"<p>Having a complete view of your workload means understanding it at all tiers, including how your end users experience it. Measuring, quantifying, and understanding when your objectives are at risk from a poor user experience is just as important as watching for free disk space or CPU utilization - if not more important!</p> <p>If your workloads are ones that interact directly with the end user (such as any application served as a web site or mobile app) then Real User Monitoring monitors not just the \"last mile\" of delivery to the user, but how they actually have experienced your application. Ultimately, none of the observability journey matters if your users are unable to actually use your services.</p>"},{"location":"guides/#data-is-power-but-dont-sweat-the-small-stuff","title":"Data is power, but don't sweat the small stuff\u00b6","text":"<p>Depending on the size of your application, you may have a very large number of components to collect signals from. While doing so is important and empowering, there can be diminished returns from your efforts. This is why the best practice is to start by monitoring what matters, use this as a way to map your important integrations and critical components, and focus on the right details.</p>"},{"location":"guides/#include-observability-from-day-one","title":"Include observability from day one\u00b6","text":"<p>Like security, observability should not be an afterthought to your development or operations. The best practice is to put observability early in your planning, just like security, which creates a model for people to work with and reduces opaque corners of your application. Adding transaction tracing after major development work is done takes time, even with auto-instrumentation. The effort returns far greater returns! But doing so late in your development cycle may create some rework.</p> <p>Rather than bolting observability in your workload later one, use it to help accelerate your work. Proper logging, metric, and trace collection enables faster application development, fosters good practices, and lays the foundation for rapid problem solving going forward.</p> <ol> <li>Amazon uses the working backwards process extensively as a way to obsession over our customers and their outcomes, and we highly recommend that anyone working on observability solutions work backwards from their own objectives in the same way. You can read more about working backwards on Werner Vogels's blog. \u21a9</li> </ol>"},{"location":"guides/choosing-a-tracing-agent/","title":"Choosing a tracing agent\u00b6","text":""},{"location":"guides/choosing-a-tracing-agent/#choose-the-right-agent","title":"Choose the right agent\u00b6","text":"<p>AWS directly supports two toolsets for trace collection (plus our wealth of observability partners:</p> <ul> <li>The AWS Distro for OpenTelemetry, commonly called ADOT</li> <li>The X-Ray SDKs and daemon</li> </ul> <p>The selection of which tool or tools to use is a principal decision you must make as you evolve your observability solution. These tools are not mutually-exclusive, and you can mix them together as necessary. And there is a best practice for making this selection. However, first you should understand the current state of OpenTelemetry (OTEL).</p> <p>OTEL is the current industry standard specification for observabillity signalling, and contains definitions for each of the three core signal types: metrics, traces, and logs. However, OTEL has not always existed and has evolved out of earlier specifications such as OpenMetrics and OpenTracing. Observability vendors began openly supporting OpenTelemetry Line Protocol (OTLP) in recent years.</p> <p>AWS X-Ray and CloudWatch pre-date the OTEL specification, as do other leading observability solutions. However, the AWS X-Ray service readily accepts OTEL traces using ADOT. ADOT has the integrations already built into it to emit telemetry into X-Ray directly, as well as to other ISV solutions.</p> <p>Any transaction tracing solution requires an agent and an integration into the underlying application in order to collect signals. And this, in turn, creates technical debt in the form of libraries that must be tested, maintained, and upgraded, as well as possibly retooling if you choose to change your solution in the future.</p> <p>The SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. ADOT is part of a broader industry solution in which X-Ray is only one of many tracing solutions. You can implement end-to-end tracing in X-Ray using either approach, but it\u2019s important to understand the differences in order to determine the most useful approach for you.</p> <p>Success</p> <p>We recommend instrumenting your application with the AWS Distro for OpenTelemetry if you need the following:</p> <ul> <li>The ability to send traces to multiple different tracing backends without having to re-instrument your code. For example, of you wish to shift from using the X-Ray console to Zipkin, then only configuration of the collector would change, leaving your applicaiton code untouched.</li> <li>Support for a large number of library instrumentations for each language, maintained by the OpenTelemetry community.</li> </ul> <p>Success</p> <p>We recommend choosing an X-Ray SDK for instrumenting your application if you need the following:</p> <ul> <li>A tightly integrated single-vendor solution.</li> <li>Integration with X-Ray centralized sampling rules, including the ability to configure sampling rules from the X-Ray console and automatically use them across multiple hosts, when using Node.js, Python, Ruby, or .NET</li> </ul>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/","title":"Collecting system metrics with Container Insights\u00b6","text":"<p>System metrics pertain to low-level resources that include physical components on a server such as CPU, memory, disks and network interfaces. Use CloudWatch Container Insights to collect, aggregate, and summarize system metrics from containerized applications deployed to Amazon ECS. Container Insights also provides diagnostic information, such as container restart failures, to help isolate issues and resolve them quickly. It is available for Amazon ECS clusters deployed on EC2 and Fargate.</p> <p>Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, service and task level as CloudWatch metrics.</p> <p>Note</p> <p>For Container Insights metrics to appear in CloudWatch, you must enable Container Insights on your Amazon ECS clusters. This can be done either at the account level or at the individual cluster level. To enable at the account level, use the following AWS CLI command:</p> <pre><code>aws ecs put-account-setting --name \"containerInsights\" --value \"enabled\n</code></pre> <p>To enable at the individual cluster level, use the following AWS CLI command:</p> <pre><code>aws ecs update-cluster-settings --cluster $CLUSTER_NAME --settings name=containerInsights,value=enabled\n</code></pre>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/#collecting-cluster-level-and-service-level-metrics","title":"Collecting cluster-level and service-level metrics\u00b6","text":"<p>By default, CloudWatch Container Insights collects metrics at the task, service and cluster level. The Amazon ECS agent collects these metrics for each task on an EC2 container instance (for both ECS on EC2 and ECS on Fargate) and sends them to CloudWatch as performance log events. You don't need to deploy any agents to the cluster. These log events from which the metrics are extracted are collected under the CloudWatch log group named /aws/ecs/containerinsights/$CLUSTER_NAME/performance. The complete list of metrics extracted from these events are documented here. The metrics that Container Insights collects are readily viewable in pre-built dashboards available in the CloudWatch console by selcting Container Insights from the navigation page and then selecting performance monitoring from the dropdown list. They are also viewable in the Metrics section of the CloudWatch console.</p> <p></p> <p>Note</p> <p>If you're using Amazon ECS on an Amazon EC2 instance, and you want to collect network and storage metrics from Container Insights, launch that instance using an AMI that includes Amazon ECS agent version 1.29.</p> <p>Warning</p> <p>Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing, see Amazon CloudWatch Pricing</p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/#collecting-instance-level-metrics","title":"Collecting instance-level metrics\u00b6","text":"<p>Deploying the CloudWatch agent to an Amazon ECS cluster hosted on EC2, allows you to collect instance-level metrics from the cluster. The agent is deployed as a daemon service and sends instance-level metrics as performance log events from each EC2 container instance in the cluster. The complete list of instance-level extracted from these events are documented here</p> <p>Info</p> <p>Steps to deploy the CloudWatch agent to an Amazon ECS cluster to collect instance-level metrics are documented in the Amazon CloudWatch User Guide. Note that this option is not availavble for Amazon ECS clusters that are hosted on Fargate.</p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-1/#analyzing-performance-log-events-with-logs-insights","title":"Analyzing performance log events with Logs Insights\u00b6","text":"<p>Container Insights collects metrics by using performance log events with embedded metric format. Each log event may contain performance data observed on system resources such as CPU and memory or ECS resources such as tasks and services. Examples of performance log events that Container Insights collects from an Amazon ECS at the cluster, service, task and container level are listed here. CloudWatch generates metrics based only on some of the performance data in these log events. But you can use these log events to perform a deeper analysis of the performance data using CloudWatch Logs Insights queries.</p> <p>The user interface to run Logs Insights queries is available in the CloudWatch console by selecting Logs Insights from the navigation page. When you select a log group, CloudWatch Logs Insights automatically detects fields in the performance log events in the log group and displays them in Discovered fields in the right pane. The results of a query execution are displayed as a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range.</p> <p></p> <p>Info</p> <p>Here's a sample Logs Insights query to display container-level metrics for CPU and memory usage.</p> <pre><code>stats avg(CpuUtilized) as CPU, avg(MemoryUtilized) as Mem by TaskId, ContainerName | sort Mem, CPU desc\n</code></pre>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/","title":"Collecting service metrics with Container Insights\u00b6","text":"<p>Service metrics are applicaton-level metrics that are captured by adding instrumentation to your code. These metrics can be captured from an application using two different approaches.</p> <ol> <li>Push approach: Here, an application sends the metrics data directly to a destination. For example, using the CloudWatch PutMetricData API, an application can publish metric data points to CloudWatch. An application may also send the data via gRPC or HTTP using the OpenTelemetry Protocol (OTLP) to an agent such as the OpenTelemetry Collector. The latter will then send the data the metrics data to the final destination.</li> <li>Pull approach: Here, the application exposes the metrics data at an HTTP endpoint in a pre-defined format. The data are then scraped by an agent that has access to this endpoint and then sent to the destination.</li> </ol> <p></p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/#cloudwatch-container-insights-monitoring-for-prometheus","title":"CloudWatch Container Insights monitoring for Prometheus\u00b6","text":"<p>Prometheus is a popular open-source systems monitoring and alerting toolkit. It has emerged as the de facto standard for collecting metrics using the pull approach from containerized applications. To capture metrics using Prometheus, you will have to first instrument your application code using the Prometheus client library which is available in all the major programming languages. Metrics are usually exposed by the application over HTTP, to be read by the Prometheus server. When Prometheus server scrapes your applications's HTTP endpoint, the client library sends the current state of all tracked metrics to the server. The server can either store the metrics in a local storage that it manages or send the metrics data to a remote destination such as CloudWatch.</p> <p>CloudWatch Container Insights monitoring for Prometheus enables you to leverage the capabilities of Prometheus in an Amazon ECS cluster. It is available for Amazon ECS clusters deployed on EC2 and Fargate The CloudWatch agent can be used as a drop-in replacement for a Prometheus server, reducing the number of monitoring tools required to improve observability. It automates the discovery of Prometheus metrics from containerized applications deployed to Amazon ECS and sends the metrics data to CloudWatch as performance log events.</p> <p>Info</p> <p>Steps to deploy the CloudWatch agent with Prometheus metrics collection on an Amazon ECS cluster are documented in the Amazon CloudWatch User Guide</p> <p>Warning</p> <p>Metrics collected by Container Insights monitoring for Prometheus are charged as custom metrics. For more information about CloudWatch pricing, see Amazon CloudWatch Pricing</p>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/#autodiscovery-of-targets-on-amazon-ecs-clusters","title":"Autodiscovery of targets on Amazon ECS clusters\u00b6","text":"<p>The CloudWatch agent supports the standard Prometheus scrape configurations under the scrape_config section in the Prometheus documentation. Prometheus supports both static and dynamic discovery of scraping targets using one of the dozens of supported service-discovery mechanisms. . As Amazon ECS does not have any built-in service discovery mechanism, the agent relies on Prometheus' support for file-based discovery of targets. To setup the agent for file-based discovery of targets, the agent needs two configuration parameters, which are both defined in the task definition used for launching the agent. You can customize these parameters to have granular control over the metrics collected by the agent.</p> <p>The first parameter contains Prometheus global configuration that looks like the following sample:</p> <pre><code>global:\n  scrape_interval: 30s\n  scrape_timeout: 10s\nscrape_configs:\n  - job_name: cwagent_ecs_auto_sd\n    sample_limit: 10000\n    file_sd_configs:\n      - files: [ \"/tmp/cwagent_ecs_auto_sd.yaml\" ] \n</code></pre> <p>The second parameter contains configuration that helps the agent discover scraping targets. The agent periodically makes API calls to Amazon ECS to retrieve the metadata of the running ECS tasks that match the task definition patterns defined in the ecs_service_discovery section of this configurtion. All discovered targets are written into the result file /tmp/cwagent_ecs_auto_sd.yaml that resides on the file system mounted to CloudWatch agent container. The sample configuration below will result in the agent scraping metrics from all tasks that are named with the prefix BackendTask. Refer to the detaild guide for autodiscovery of targets in an Amazon ECS Cluster.</p> <pre><code>{\n   \"logs\":{\n      \"metrics_collected\":{\n         \"prometheus\":{\n            \"log_group_name\":\"/aws/ecs/containerinsights/{ClusterName}/prometheus\"\n            \"prometheus_config_path\":\"env:PROMETHEUS_CONFIG_CONTENT\",\n            \"ecs_service_discovery\":{\n               \"sd_frequency\":\"1m\",\n               \"sd_result_file\":\"/tmp/cwagent_ecs_auto_sd.yaml\",\n               \"task_definition_list\":[\n                  {\n                     \"sd_job_name\":\"backends\",\n                     \"sd_metrics_ports\":\"3000\",\n                     \"sd_task_definition_arn_pattern\":\".*:task-definition/BackendTask:[0-9]+\",\n                     \"sd_metrics_path\":\"/metrics\"\n                  }\n               ]\n            },\n            \"emf_processor\":{\n               \"metric_declaration\":[\n                  {\n                     \"source_labels\":[\n                        \"job\"\n                     ],\n                     \"label_matcher\":\"^backends$\",\n                     \"dimensions\":[\n                        [\n                           \"ClusterName\",\n                           \"TaskGroup\"\n                        ]\n                     ],\n                     \"metric_selectors\":[\n                        \"^http_requests_total$\"\n                     ]\n                  }\n               ]\n            }\n         }\n      },\n      \"force_flush_interval\":5\n   }\n}\n</code></pre>"},{"location":"guides/containers/aws-native/ecs/best-practices-metrics-collection-2/#importing-prometheus-metrics-into-cloudwatch","title":"Importing Prometheus metrics into CloudWatch\u00b6","text":"<p>The metrics collected by the agent are sent to CloudWatch as performance log events based on the filtering rules specified in metric_declaration section of the configuration. This section is also used to specify the array of logs with embedded metric format to be generated. The sample configuration above will generate log events, as shown below, only for a metric named http_requests_total with the label job:backends. Using this data, CloudWatch will create the metric http_requests_total under the CloudWatch namespace ECS/ContainerInsights/Prometheus with the dimensions ClusterName and TaskGroup.</p> <pre><code>{\n   \"CloudWatchMetrics\":[\n      {\n         \"Metrics\":[\n            {\n               \"Name\":\"http_requests_total\"\n            }\n         ],\n         \"Dimensions\":[\n            [\n               \"ClusterName\",\n               \"TaskGroup\"\n            ]\n         ],\n         \"Namespace\":\"ECS/ContainerInsights/Prometheus\"\n      }\n   ],\n   \"ClusterName\":\"ecs-sarathy-cluster\",\n   \"LaunchType\":\"EC2\",\n   \"StartedBy\":\"ecs-svc/4964126209508453538\",\n   \"TaskDefinitionFamily\":\"BackendAlarmTask\",\n   \"TaskGroup\":\"service:BackendService\",\n   \"TaskRevision\":\"4\",\n   \"Timestamp\":\"1678226606712\",\n   \"Version\":\"0\",\n   \"container_name\":\"go-backend\",\n   \"exported_job\":\"storebackend\",\n   \"http_requests_total\":36,\n   \"instance\":\"10.10.100.191:3000\",\n   \"job\":\"backends\",\n   \"path\":\"/popular/category\",\n   \"prom_metric_type\":\"counter\"\n}\n</code></pre>"},{"location":"guides/ec2/ec2-monitoring/","title":"Amazon EC2 Monitoring and Observability\u00b6","text":""},{"location":"guides/ec2/ec2-monitoring/#introduction","title":"Introduction\u00b6","text":"<p>Continuous Monitoring &amp; Observability increases agility, improves customer experience and reduces risk of the cloud environment. According to Wikipedia, Observability is a measure of how well internal states of a system can be inferred from the knowledge of its external outputs. The term observability itself originates from the field of control theory, where it basically means that you can infer the internal state of the components in a system by learning about the external signals/outputs it is producing.</p> <p>The difference between Monitoring and Observability is that Monitoring tells you whether a system is working or not, while Observability tells you why the system isn\u2019t working. Monitoring is usually a reactive measure whereas the goal of Observability is to be able to improve your Key Performance Indicators in a proactive manner. A system cannot be controlled or optimized unless it is observed. Instrumenting workloads through collection of metrics, logs, or traces and gaining meaningful insights &amp; detailed context using the right monitoring and observability tools help customers control and optimize the environment.</p> <p></p> <p>AWS enables customers to transform from monitoring to observability so that they can have full end-to-end service visibility. In this article we focus on Amazon Elastic Compute Cloud (Amazon EC2) and the best practices for improving the monitoring and observability of the service in AWS Cloud environment through AWS native and open-source tools.</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-ec2","title":"Amazon EC2\u00b6","text":"<p>Amazon Elastic Compute Cloud (Amazon EC2) is a highly scalable compute platform in Amazon Web Services (AWS) Cloud. Amazon EC2 eliminates the need for up front hardware investment, so customers can develop and deploy applications faster while paying just for what they use. Some of the key features that EC2 provide are virtual computing environments called Instances, pre-configured templates of Instances called Amazon Machine Images, various configurations of resources like CPU, Memory, Storage and Networking capacity available as Instance Types.</p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-and-observability-using-aws-native-tools","title":"Monitoring and Observability using AWS Native Tools\u00b6","text":""},{"location":"guides/ec2/ec2-monitoring/#amazon-cloudwatch","title":"Amazon CloudWatch\u00b6","text":"<p>Amazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS, hybrid, and on-premises applications and infrastructure resources. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. It also provides a unified view of AWS resources, applications, and services that run on AWS and on-premises servers. CloudWatch helps you gain system-wide visibility into resource utilization, application performance, and operational health.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#unified-cloudwatch-agent","title":"Unified CloudWatch Agent\u00b6","text":"<p>The Unified CloudWatch Agent is an open-source software under the MIT license which supports most operating systems utilizing x86-64 and ARM64 architectures. The CloudWatch Agent helps collect system-level metrics from Amazon EC2 Instances &amp; on-premise servers in a hybrid environment across operating systems, retrieve custom metrics from applications or services and collect logs from Amazon EC2 instances and on-premises servers.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#installing-cloudwatch-agent-on-amazon-ec2-instances","title":"Installing CloudWatch Agent on Amazon EC2 Instances\u00b6","text":""},{"location":"guides/ec2/ec2-monitoring/#command-line-install","title":"Command Line Install\u00b6","text":"<p>The CloudWatch Agent can be installed through the command line. The required package for various architectures and various operating systems are available for download. Create the necessary IAM role which provides permissions for CloudWatch agent to read information from the Amazon EC2 instance and write it to CloudWatch. Once the required IAM role is created, you can install and run the CloudWatch agent on the required Amazon EC2 Instance.</p> <p>References</p> <p>Documentation: Installing the CloudWatch agent using the command line</p> <p>AWS Observability Workshop: Setup and install CloudWatch agent</p>"},{"location":"guides/ec2/ec2-monitoring/#installation-through-aws-systems-manager","title":"Installation through AWS Systems Manager\u00b6","text":"<p>The CloudWatch Agent can also be installed through AWS Systems Manager. Create the necessary IAM role which provides permissions for CloudWatch agent to read information from the Amazon EC2 instance and write it to CloudWatch &amp; communicate with AWS Systems Manager. Before installing the CloudWatch agent on the EC2 instances, install or update the SSM agent on the required EC2 instances. The CloudWatch agent can be downloaded through the AWS Systems Manager. JSON Configuration file can be created to specify what metrics (including custom metrics), logs are to be collected. Once the required IAM role is created &amp; the configuration file is created, you can install and run the CloudWatch agent on the required Amazon EC2 Instances.</p> <p>References</p> <p>References: Documentation: Installing the CloudWatch agent using AWS Systems Manager</p> <p>AWS Observability Workshop: Install CloudWatch agent using AWS Systems Manager Quick Setup</p> <p>Related Blog Article: Amazon CloudWatch Agent with AWS Systems Manager Integration \u2013 Unified Metrics &amp; Log Collection for Linux &amp; Windows</p> <p>YouTube Video: Collect Metrics and Logs from Amazon EC2 instances with the CloudWatch Agent</p>"},{"location":"guides/ec2/ec2-monitoring/#installing-cloudwatch-agent-on-on-premise-servers-in-hybrid-environment","title":"Installing CloudWatch Agent on on-premise servers in hybrid environment.\u00b6","text":"<p>In hybrid customer environments, where servers are on-premisis as well as in the cloud. A similar approach of can be taken to accomplish unified observability in Amazon CloudWatch. The CloudWatch agent can be directly downloaded from Amazon S3 or through AWS Systems Manager. Create an IAM User for the on-premise server to send data to Amazon CloudWatch. Install and Start the Agent on the on-premise servers.</p> <p>Reference</p> <p>Documentation: Installing the CloudWatch agent on on-premises servers</p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-of-amazon-ec2-instances-using-amazon-cloudwatch","title":"Monitoring of Amazon EC2 Instances using Amazon CloudWatch\u00b6","text":"<p>A key aspect of maintaining the reliability, availability, and performance of your Amazon EC2 Instances and your applications is through continuous monitoring. With CloudWatch Agent installed on the required Amazon EC2 instances, monitoring the health of the instances and their performance is necessary to maintain a stable environment. As a baseline, items like CPU utilization, Network utilization, Disk performance, Disk Reads/Writes, Memory utilization, disk swap utilization, disk space utilization, page file utilization, and log collection of EC2 Instances are recommended.</p>"},{"location":"guides/ec2/ec2-monitoring/#basic-detailed-monitoring","title":"Basic &amp; Detailed Monitoring\u00b6","text":"<p>Amazon CloudWatch collects and processes raw data from Amazon EC2 into readable near real-time metrics. By default, Amazon EC2 sends metric data to CloudWatch in 5-minute periods as Basic Monitoring for an instance. To send metric data for your instance to CloudWatch in 1-minute periods, detailed monitoring can be enabled on the instance.</p>"},{"location":"guides/ec2/ec2-monitoring/#automated-manual-tools-for-monitoring","title":"Automated &amp; Manual Tools for Monitoring\u00b6","text":"<p>AWS provides two types of tools, automated and manual that help customers monitor their Amazon EC2 and report back when something is wrong. Some of these tools require a little configuration and a few require manual intervention. Automated Monitoring tools include AWS System status checks, Instance status checks, Amazon CloudWatch alarms, Amazon EventBridge, Amazon CloudWatch Logs, CloudWatch agent, AWS Management Pack for Microsoft System Center Operations Manager. Manual monitoring tools include Dashboards which we\u2019ll look in detail a separate section below in this article.</p> <p>Reference</p> <p>Documentation: Automated and manual monitoring</p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-from-amazon-ec2-instances-using-cloudwatch-agent","title":"Metrics from Amazon EC2 Instances using CloudWatch Agent\u00b6","text":"<p>Metrics are the fundamental concept in CloudWatch. A metric represents a time-ordered set of data points that are published to CloudWatch. Think of a metric as a variable to monitor, and the data points as representing the values of that variable over time. For example, the CPU usage of a particular EC2 instance is one metric provided by Amazon EC2.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#default-metrics-using-cloudwatch-agent","title":"Default Metrics using CloudWatch Agent\u00b6","text":"<p>Amazon CloudWatch collects metrics from Amazon EC2 instance which can be viewed through AWS Management Console, AWS CLI, or an API. The available metrics are data points which are covered for 5 minute interval through Basic Monitoring or at a 1 minute interval for detailed monitoring (if turned on).</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#custom-metrics-using-cloudwatch-agent","title":"Custom Metrics using CloudWatch Agent\u00b6","text":"<p>Customers can also publish their own custom metrics to CloudWatch using the API or CLI through standard resolution of 1 minute granularity or high resolution granularity down to 1 sec interval. The unified CloudWatch agent supports retrieval of custom metrics through StatsD and collectd.</p> <p>Custom metrics from applications or services can be retrieved using the CloudWatch agent with StatsD protocol. StatsD is a popular open-source solution that can gather metrics from a wide variety of applications. StatsD is especially useful for instrumenting own metrics, which supports both Linux and Windows based servers.</p> <p>Custom metrics from applications or services can also be retrieved using the CloudWatch agent with the collectd protocol, which is a popular open-source solution supported only on Linux Servers with plugins that can gather system statistics for a wide variety of applications. By combining the system metrics that the CloudWatch agent can already collect with the additional metrics from collectd, you can better monitor, analyze, and troubleshoot your systems and applications.</p>"},{"location":"guides/ec2/ec2-monitoring/#additional-custom-metrics-using-cloudwatch-agent","title":"Additional Custom Metrics using CloudWatch Agent\u00b6","text":"<p>The CloudWatch agent supports collecting custom metrics from your EC2 instances. A few popular examples are:</p> <ul> <li>Network performance metrics for EC2 instances running on Linux that use the Elastic Network Adapter (ENA).</li> <li>Nvidia GPU metrics from Linux servers.</li> <li>Process metrics using procstat plugin from individual processes on Linux &amp; Windows servers.</li> </ul>"},{"location":"guides/ec2/ec2-monitoring/#logs-from-amazon-ec2-instances-using-cloudwatch-agent","title":"Logs from Amazon EC2 Instances using CloudWatch Agent\u00b6","text":"<p>Amazon CloudWatch Logs helps customers monitor and troubleshoot systems and applications in near real time using existing system, application and custom log files. To collect logs from Amazon EC2 Instances and on-premise servers to CloudWatch, the unified CloudWatch Agent needs to be installed. The latest unified CloudWatch Agent is recommended, since it can collect both logs and advanced metrics. It also supports a variety of operating systems. If the instance uses Instance Metadata Service Version 2 (IMDSv2) then the unified agent is required.</p> <p></p> <p>The logs collected by the unified CloudWatch agent are processed and stored in Amazon CloudWatch Logs. Logs can be collected from Windows or Linux Servers and from both Amazon EC2 and on-premise servers. The CloudWatch agent configuration wizard can be used to setup the config JSON file which defines the setup of the CloudWatch agent.</p> <p></p> <p>Reference</p> <p>AWS Observability Workshop: Logs</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-ec2-instance-events","title":"Amazon EC2 Instance Events\u00b6","text":"<p>An event indicates a change in your AWS environment. AWS resources and applications can generate events when their state changes. CloudWatch Events provides a near real-time stream of system events that describe changes to your AWS resources and applications. For example, Amazon EC2 generates an event when the state of an EC2 instance changes from pending to running. Customers can also generate custom application-level events and publish them to CloudWatch Events.</p> <p>Customers can monitor the status of Amazon EC2 Instances by viewing status checks and scheduled events. A status check provides the results from automated checks performed by Amazon EC2. These automated checks detect whether specific issues are affecting the instances. The status check information, together with the data provided by Amazon CloudWatch, gives detailed operational visibility into each of the instances.</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-eventbridge-rule-for-amazon-ec2-instance-events","title":"Amazon EventBridge Rule for Amazon EC2 Instance Events\u00b6","text":"<p>Amazon CloudWatch Events can use Amazon EventBridge to automate system events to respond automatically for actions such as resource changes or issues. Events from AWS services including Amazon EC2 are delivered to CloudWatch Events in near real time and customers can create EventBridge rules to take appropriate actions when an event matches a rule. Actions can be, Invoke an AWS Lambda function, Invoke Amazon EC2 Run Command, Relay the event to Amazon Kinesis Data Streams, Activate an AWS Step Functions state machine, Notify an Amazon SNS topic, Notify an Amazon SQS queue, piping to internal or external incident response application or SIEM tool.</p> <p>Reference</p> <p>AWS Observability Workshop: Incident Response - EventBridge Rule</p>"},{"location":"guides/ec2/ec2-monitoring/#amazon-cloudwatch-alarms-for-amazon-ec2-instances","title":"Amazon CloudWatch Alarms for Amazon EC2 Instances\u00b6","text":"<p>Amazon CloudWatch alarms can watch a metric over a time period you specify, and perform one or more actions based on the value of the metric relative to a given threshold over a number of time periods. An alarm invokes actions only when the alarmchanges state. The action can be a notification sent to an Amazon Simple Notification Service (Amazon SNS) topic or Amazon EC2 Auto Scaling or take other appropriate actions like stop, terminate, reboot, or recover an EC2 instance.</p> <p></p> <p>Once the alarm is triggered an Email notification is sent to an SNS Topic as an action.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-for-auto-scaling-instances","title":"Monitoring for Auto Scaling Instances\u00b6","text":"<p>Amazon EC2 Auto Scaling helps customer ensure that you have the correct number of Amazon EC2 instances are available to handle the load for your application. Amazon EC2 Auto Scaling metrics collect information about Auto Scaling groups and are in the AWS/AutoScaling namespace. Amazon EC2 instance metrics representing CPU and other usage data from Auto Scaling instances are in the AWS/EC2 namespace.</p>"},{"location":"guides/ec2/ec2-monitoring/#dashboarding-in-cloudwatch","title":"Dashboarding in CloudWatch\u00b6","text":"<p>Getting to know the inventory details of resources in AWS accounts, the resources performance and health checks is important for a stable resource management. Amazon CloudWatch dashboards are customizable home pages in the CloudWatch console that you can be used to monitor your resources in a single view, even those resources that are spread across different Regions. There are ways to get a good view and details of the Amazon EC2 Instances that are available</p>"},{"location":"guides/ec2/ec2-monitoring/#automatic-dashboards-in-cloudwatch","title":"Automatic Dashboards in CloudWatch\u00b6","text":"<p>Automatic Dashboards are available in all AWS public regions which provides an aggregated view of the health and performance of all AWS resources including Amazon EC2 instances under CloudWatch. This helps customers quickly get started with monitoring, resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Automatic Dashboards are pre-built with AWS service recommended best practices, remain resource aware, and dynamically update to reflect the latest state of important performance metrics.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#custom-dashboards-in-cloudwatch","title":"Custom Dashboards in CloudWatch\u00b6","text":"<p>With Custom Dashboards Customers can create as many additional dashboards as they want with different widgets and customize it accordingly . Dashboards can be configured for cross-region and cross account view and can be added to a favorites list.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#resource-health-dashboards-in-cloudwatch","title":"Resource Health Dashboards in CloudWatch\u00b6","text":"<p>Resource Health in CloudWatch ServiceLens is a fully managed solution that customers can use to automatically discover, manage, and visualize the health and performance of Amazon EC2 hosts across their applications. Customers can visualize the health of their hosts by performance dimension such as CPU or memory, and slice and dice hundreds of hosts in a single view using filters such as instance type, instance state, or security groups. It enables a side-by-side comparison of a group of EC2 hosts and provides granular insights into an individual host.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-and-observability-using-open-source-tools","title":"Monitoring And Observability using Open Source Tools\u00b6","text":""},{"location":"guides/ec2/ec2-monitoring/#monitoring-of-amazon-ec2-instances-using-aws-distro-for-opentelemetry","title":"Monitoring of Amazon EC2 Instances using AWS Distro for OpenTelemetry\u00b6","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a secure, production-ready, AWS-supported distribution of the OpenTelemetry project. Part of the Cloud Native Computing Foundation, OpenTelemetry provides open source APIs, libraries, and agents to collect distributed traces and metrics for application monitoring. With AWS Distro for OpenTelemetry, customers can instrument applications just once to send correlated metrics and traces to multiple AWS and Partner monitoring solutions.</p> <p></p> <p>AWS Distro for OpenTelemetry (ADOT) provides a distributed monitoring framework that enables correlating data for monitoring application performance and health in an easy way which is critical for greater service visibility and maintenance.</p> <p>The key components of ADOT are SDKs, auto-instrumentation agents, collectors and exporters to send data to back-end services.</p> <p>OpenTelemetry SDK: To enable the collection of AWS resource-specific metadata, support to the OpenTelemetry SDKs for the X-Ray trace format and context. OpenTelemetry SDKs now correlate ingested trace and metrics data from AWS X-Ray and CloudWatch.</p> <p>Auto-instrumentation agent: Support in the OpenTelemetry Java auto-instrumentation agent are added for AWS SDK and AWS X-Ray trace data.</p> <p>OpenTelemetry Collector: The collector in the distribution is built using the upstream OpenTelemetry collector. Added AWS-specific exporters to the upstream collector to send data to AWS services including AWS X-Ray, Amazon CloudWatch and Amazon Managed Service for Prometheus.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-traces-through-adot-collector-amazon-cloudwatch","title":"Metrics &amp; Traces through ADOT Collector &amp; Amazon CloudWatch\u00b6","text":"<p>AWS Distro for OpenTelemetry (ADOT) Collector along with the CloudWatch agent can be installed side-by-side on Amazon EC2 Instance and OpenTelemetry SDKs can be used to collect application traces &amp; metrics from your workloads running on Amazon EC2 Instances.</p> <p>To support OpenTelemetry metrics in Amazon CloudWatch, AWS EMF Exporter for OpenTelemetry Collector converts OpenTelemetry format metrics to CloudWatch Embedded Metric Format(EMF) which enables applications integrated in OpenTelemetry metrics to be able to send high-cardinality application metrics to CloudWatch. The X-Ray exporter allows traces collected in an OTLP format to be exported to AWS X-ray.</p> <p></p> <p>ADOT Collector on Amazon EC2 can be installed through AWS CloudFormation or using AWS Systems Manager Distributor to collect application metrics.</p>"},{"location":"guides/ec2/ec2-monitoring/#monitoring-of-amazon-ec2-instances-using-prometheus","title":"Monitoring of Amazon EC2 Instances using Prometheus\u00b6","text":"<p>Prometheus is a standalone open-source project and maintained independently for systems monitoring and alerting. Prometheus collects and stores metrics as time series data, i.e. metrics information is stored with the timestamp at which it was recorded, alongside optional key-value pairs called labels.</p> <p></p> <p>Prometheus is configured via command line flags and all the configuration details are maintained in the prometheus.yaml file. The 'scrape_config' section within the configuration file specifies the targets and parameters specifying how to scrape them. Prometheus Service Discovery (SD) is a methodology of finding endpoints to scrape for metrics. Amazon EC2 service discovery configurations allow retrieving scrape targets from AWS EC2 instances are configured in the <code>ec2_sd_config</code>.</p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-through-prometheus-amazon-cloudwatch","title":"Metrics through Prometheus &amp; Amazon CloudWatch\u00b6","text":"<p>The CloudWatch agent on EC2 instances can be installed &amp; configured with Prometheus to scrape metrics for monitoring in CloudWatch. This can be helpful to customers who prefer container workloads on EC2 and require custom metrics that are compatible with open source Prometheus monitoring. Installation of CloudWatch Agent can be done by following the steps explained in the earlier section above. The CloudWatch agent with Prometheus monitoring needs two configurations to scrape the Prometheus metrics. One is for the standard Prometheus configurations as documented in 'scrape_config' in the Prometheus documentation. The other is for the CloudWatch agent configuration.</p>"},{"location":"guides/ec2/ec2-monitoring/#metrics-through-prometheus-adot-collector","title":"Metrics through Prometheus &amp; ADOT Collector\u00b6","text":"<p>Customers can choose to have an all open-source setup for their observability needs. For which, AWS Distro for OpenTelemetry (ADOT) Collector can be configured to scrape from a Prometheus-instrumented application and send the metrics to Prometheus Server. There are three OpenTelemetry components involved in this flow, that are the Prometheus Receiver, the Prometheus Remote Write Exporter, and the Sigv4 Authentication Extension. Prometheus Receiver receives metric data in Prometheus format. Prometheus Exporter exports data in Prometheus format. Sigv4 Authenticator extension provides Sigv4 authentication for making requests to AWS services.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#prometheus-node-exporter","title":"Prometheus Node Exporter\u00b6","text":"<p>Prometheus Node Exporter is an open-source time series monitoring and alerting system for cloud environments. Amazon EC2 Instances can be instrumented with Node Exporter to collect and store node-level metrics as time-series data, recording information with a timestamp. Node exporter is a Prometheus exporter which can expose variety of host metrics via URL http://localhost:9100/metrics.</p> <p></p> <p>Once the metrics are created, they can be sent to Amazon Managed Prometheus.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#streaming-logs-from-amazon-ec2-instances-using-fluent-bit-plugin","title":"Streaming Logs from Amazon EC2 Instances using Fluent Bit Plugin\u00b6","text":"<p>Fluent Bit is an open source and multi-platform log processor tool for handling data collection at scale, collecting &amp; aggregating diverse data that deal with various sources of information, variety of data formats, data reliability, security, flexible routing and multiple destinations.</p> <p></p> <p>Fluent Bit helps create an easy extension point for streaming logs from Amazon EC2 to AWS services including Amazon CloudWatch for log retention and analytics. The newly-launched Fluent Bit plugin can route logs to Amazon CloudWatch.</p>"},{"location":"guides/ec2/ec2-monitoring/#dashboarding-with-amazon-managed-grafana","title":"Dashboarding with Amazon Managed Grafana\u00b6","text":"<p>Amazon Managed Grafana is a fully managed service based on the open source Grafana project, with rich, interactive &amp; secure data visualizations to help customers instantly query, correlate, analyze, monitor, and alarm on metrics, logs, and traces across multiple data sources. Customers can create interactive dashboards and share them with anyone in their organization with an automatically scaled, highly available, and enterprise-secure service. With Amazon Managed Grafana, customers can manage user and team access to dashboards across AWS accounts, AWS regions, and data sources.</p> <p></p> <p>Amazon Managed Grafana can be added with Amazon CloudWatch as a data source by using the AWS data source configuration option in the Grafana workspace console. This feature simplifies adding CloudWatch as a data source by discovering existing CloudWatch accounts and manage the configuration of the authentication credentials that are required to access CloudWatch. Amazon Managed Grafana also supports Prometheus data sources, i.e. both self-managed Prometheus servers and Amazon Managed Service for Prometheus workspaces as data sources.</p> <p>Amazon Managed Grafana comes with a variety of panels, makes it easy to construct the right queries and customize the display properties allowing customers to create the dashboards they need.</p> <p></p>"},{"location":"guides/ec2/ec2-monitoring/#conclusion","title":"Conclusion\u00b6","text":"<p>Monitoring keeps you informed of whether a system is working properly. Observability lets you understand why the system is not working properly. Good observability allows you to answer the questions you didn't know that you needed to be aware of. Monitoring &amp; Observability paves way for measuring the internal states of a system which can be inferred from its outputs.</p> <p>Modern applications, those running on cloud in microservices, serverless and asynchronous architectures, generate large volumes of data in the form of metrics, logs, traces and events. Amazon CloudWatch along with open source tools such as Amazon Distro for OpenTelemetry, Amazon Managed Prometheus, and Amazon Managed Grafana, enable customers to collect, access, and correlate this data on a unified platform. Helping customers break down data silos so you can easily gain system-wide visibility and quickly resolve issues.</p>"},{"location":"signals/logs/","title":"Logs\u00b6","text":"<p>Logs are a series of messages that are sent by an application, or an appliance, that are represented by one or more lines of details about an event, or sometimes about the health of that application. Typically, logs are delivered to a file, though sometimes they are sent to a collector that performs analysis and aggregation. There are many full-featured log aggregators, frameworks, and products that aim to make the task of generating, ingesting, and managing log data at any volume \u2013 from megabytes per day to terabytes per hour.</p> <p>Logs are emitted by a single application at a time and usually pertain to the scope of that one application - though developers are free to have logs be as complex and nuanced as they desire. For our purposes we consider logs to be a fundamentally different signal from traces, which are composed of events from more than one application or a service, and with context about the connection between services such as response latency, service faults, request parameters etc.</p> <p>Data in logs can also be aggregate over a period of time. For example, they may be statistical (e.g. number of requests served over the previous minute). They can be structured, free-form, verbose, and in any written language.</p> <p>The primary use cases for logging are describing,</p> <ul> <li>an event, including its status and duration, and other vital statistics</li> <li>errors or warnings related to that event (e.g. stack traces, timeouts)</li> <li>application launches, start-up and shutdown messages</li> </ul> <p>Note</p> <p>Logs are intended to be immutable, and many log management systems include mechanisms to protect against, and detect attempts, to modify log data.</p> <p>Regardless of your requirements for logging, these are the best practices that we have identified.</p>"},{"location":"signals/logs/#structured-logging-is-key-to-success","title":"Structured logging is key to success\u00b6","text":"<p>Many systems will emit logs in a semi-structured format. For example, an Apache web server may write logs like this, with each line pertaining to a single web request:</p> <pre><code>192.168.2.20 - - [28/Jul/2006:10:27:10 -0300] \"GET /cgi-bin/try/ HTTP/1.0\" 200 3395\n127.0.0.1 - - [28/Jul/2006:10:22:04 -0300] \"GET / HTTP/1.0\" 200 2216\n</code></pre> <p>Whereas a Java stack trace may be a single event that spans multiple lines and is less structured:</p> <pre><code>Exception in thread \"main\" java.lang.NullPointerException\n    at com.example.myproject.Book.getTitle(Book.java:16)\n    at com.example.myproject.Author.getBookTitles(Author.java:25)\n    at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\n</code></pre> <p>And a Python error log event may look like this:</p> <pre><code>Traceback (most recent call last):\n  File \"e.py\", line 7, in &lt;module&gt;\n    raise TypeError(\"Again !?!\")\nTypeError: Again !?!\n</code></pre> <p>Of these three examples, only the first one is easily parsed by both humans and a log aggregation system. Using structured logs makes it easy to process log data quickly and effectively, giving both humans and machines the data they need to immediately find what they are looking for.</p> <p>The most commonly understood log format is JSON, wherein each component to an event is represented as a key/value pair. In JSON, the python example above may be rewritten to look like this:</p> <pre><code>{\n    \"level\", \"ERROR\"\n    \"file\": \"e.py\",\n    \"line\": 7,\n    \"error\": \"TypeError(\\\"Again !?!\\\")\"\n}\n</code></pre> <p>The use of structured logs makes your data transportable from one log system to another, simplifies development, and make operational diagnosis faster (with less errors). Also, using JSON embeds the schema of the log message along with the actual data, which enables sophisticated log analysis systems to index your messages automatically.</p>"},{"location":"signals/logs/#use-log-levels-appropriately","title":"Use log levels appropriately\u00b6","text":"<p>There are two types of logs: those that have a level and those that are a series of events. For those that have a level, these are a critical component to a successful logging strategy. Log levels vary slightly from one framework to another, but generally they follow this structure:</p> Level Description <code>DEBUG</code> Fine-grained informational events that are most useful to debug an application. These are usually of value to devlopers and are very verbose. <code>INFO</code> Informational messages that highlight the progress of the application at coarse-grained level. <code>WARN</code> Potentially harmful situations that indicate a risk to an application. These can trigger an alarm in an applicaiton. <code>ERROR</code> Error events that might still allow the application to continue running. These are likely to trigger an alarm that requires attention. <code>FATAL</code> Very severe error events that will presumably cause an application to abort. <p>Info</p> <p>Implicitly logs that have no explicit level may be considered as <code>INFO</code>, though this behaviour may vary between applications.</p> <p>Other common log levels are <code>CRITICAL</code> and <code>NONE</code>, depending on your needs, programming language, and framework. <code>ALL</code> and <code>NONE</code> are also common, though not found in every application stack.</p> <p>Log levels are crucial for informing your monitoring and observability solution about the health of your environment, and log data should easily express this data using a logical value.</p> <p>Tip</p> <p>Logging too much data at <code>WARN</code> will fill your monitoring system with data that is of limited value, and then you may lose important data in the sheer volume of messages.</p> <p></p> <p>Success</p> <p>Using a standardized log level strategy makes automation easier, and helps developers get to the root cause of issues quickly.</p> <p>Warning</p> <p>Without a standard approach to log levels, filtering your logs is a major challenge.</p>"},{"location":"signals/logs/#filter-logs-close-to-the-source","title":"Filter logs close to the source\u00b6","text":"<p>Wherever possible, reduce the volume of logs as close to the source as possible. There are many reasons to follow this best practice:</p> <ul> <li>Ingesting logs always costs time, money, and resources.</li> <li>Filtering sensitive data (e.g. personally identifiable data) from downstream systems reduces risk exposure from data leakage.</li> <li>Downstream systems may not have the same operational concerns as the sources of data. For example, <code>INFO</code> logs from an application may be of no interest to a monitoring and alerting system that watches for <code>CRITCAL</code> or <code>FATAL</code> messages.</li> <li>Log systems, and networks, need not be placed under undue stress and traffic.</li> </ul> <p>Success</p> <p>Filter your log close to the source to keep your costs down, decrease risk of data exposure, and focus each component on the things that matter.</p> <p>Tip</p> <p>Depending on your architecture, you may wish to use infrastructure as code (IaC) to deploy changes to your application and environment in one operation. This approach allows you to deploy your log filter patterns along with applications, giving them the same rigor and treatment.</p>"},{"location":"signals/logs/#avoid-double-ingestion-antipatterns","title":"Avoid double-ingestion antipatterns\u00b6","text":"<p>A common pattern that administrators pursue is copying all of their logging data into a single system with the goal querying all of their logs all from a single location. There are some manual workflow advantages to doing so, however this pattern introduces additional cost, complexity, points of failure, and operational overhead.</p> <p></p> <p>Success</p> <p>Where possible, use a combination of log levels and log filtering to avoid a wholesale propagation of log data from your environments.</p> <p>Info</p> <p>Some organizations or workloads require log shipping in order to meet regulatory requirements, store logs in a secure location, provide non-reputability, or achieve other objectives. This is a common use case for re-ingesting log data. Note that a proper application of log levels and log filtering is still appropriate to reduce the volume of superfluous data entering these log archives.</p>"},{"location":"signals/logs/#collect-metric-data-from-your-logs","title":"Collect metric data from your logs\u00b6","text":"<p>Your logs contain metrics that are just waiting to be collected! Even ISV solutions or applications that you have not written yourself will emit valuable data into their logs that you can extract meaningful insights into overall workload health from. Common examples include:</p> <ul> <li>Slow query time from databases</li> <li>Uptime from web servers</li> <li>Transaction processing time</li> <li>Counts of <code>ERROR</code> or <code>WARNING</code> events over time</li> <li>Raw count of packages that are available for upgrade</li> </ul> <p>Tip</p> <p>This data is less useful when locked in a static log file. The best practice is to identify key metric data and then publish it into your metric system where it can be correlated with other signals.</p>"},{"location":"signals/logs/#log-to-stdout","title":"Log to <code>stdout</code>\u00b6","text":"<p>Where possible, applications shouould log to <code>stdout</code> rather than to a fixed location such as a file or socket. This enables log agents to collect and route your log events based on rules that make sense for your own observability solution. While not possible for all applications, this is the best practice for containerized workloads.</p> <p>Note</p> <p>While applications should be generic and simple in their logging practices, remaining loosely coupled from logging solutions, the transmission of log data does still require a log collector to send data from <code>stdout</code> to a file. The important concept is to avoid application and business logic being dependant on your logging infrastructure - in other words, you should work to separate your concerns.</p> <p>Success</p> <p>Decoupling your application from your log management lets you adapt and evolve your solution without code changes, thereby minimizing the potential blast radius of changes made to your environment.</p>"},{"location":"signals/logs/metrics/","title":"Metrics\u00b6","text":"<p>Metrics are a series of numerical values that are kept in order with the time that they are created. They are used to track everything from the number of servers in your environment, their disk usage, number of requests they handle per second, or the latency in completing these requests.</p> <p>But metrics are not limited to infrastructure or application monitoring. Rather, they can be used for any kind of business or workload to track sales, call queues, and customer satisfaction. In fact, metrics are most useful when combining both operational data and business metrics, giving a well-rounded view and observable system.</p> <p>It might be worth looking into the OpenTelemetry documentation page that provides some additional context on Metrics.</p>"},{"location":"signals/logs/metrics/#know-your-key-performance-indicatorskpis-and-measure-them","title":"Know your Key Performance Indicators(KPIs), and measure them","text":"<p>The most important thing with metrics is to measure the right things. And what those are will be different for everyone. An e-commerce application may have sales per hour as a critical KPI, whereas a bakery would like be more interested in the number of croissants made per day.</p> <p>Warning</p> <p>There is no singular, entirely complete, and comprehensive source for your business KPIs. You must understand your project or application well enough to know what your output goals are.</p> <p>Your first step is to name your high-level goals, and most likely those goals are not expressed as a single metric that comes from your infrastructure alone. In the e-commerce example above, once you identify the meta goal which is measuring sales per hour, you then can backtrack to detailed metrics such as time spent to search a product before purchase, time taken to complete the checkout process, latency of product search results and so on. This will guide us to be intentional about collecting relevant information to observe the system.</p> <p>Success</p> <p>Having identified your KPIs, you can now work backwards to see what metrics in your workload impact them.</p>"},{"location":"signals/logs/metrics/#correlate-with-operational-metric-data","title":"Correlate with operational metric data\u00b6","text":"<p>If high CPU utilization on your web server causes slow response times, which in turn makes for dissatisfied customers and ultimately lower revenue, then measuring your CPU utilization has a direct impact on your business outcomes and should absolutely be measured!</p> <p>Or conversely, if you have an application that performs batch processing on ephemeral cloud resources (such as an Amazon EC2 fleet, or similar in other cloud provider environments), then you may want to have CPU as utilized as possible in order to accomplish the most cost-effective means of completing the batch.</p> <p>In either case, you need to have your operational data (e.g. CPU utilization) be in the same system as your business metrics so you can correlate the two.</p> <p>Success</p> <p>Store your business metrics and operational metrics in a system where you can correlate them together and draw conclusions based on observed impacts to both.</p>"},{"location":"signals/logs/metrics/#know-what-good-looks-like","title":"Know what good looks like","text":"<p>Understanding what a healthy baseline is can be challenging. Many people have to stress test their workloads to understand what healthy metrics look like. However, depending on your needs you may be able to observe existing operational metrics to draw safe conclusions about healthy thresholds.</p> <p>A healthy workload is one that has a balance of meeting your KPI objectives while remaining resilient, available, and cost-effective.</p> <p>Success</p> <p>Your KPIs must have an identified healthy range so you can create alarms when performance falls below, or above, what is required.</p>"},{"location":"signals/logs/metrics/#use-anomaly-detection-algorithms","title":"Use anomaly detection algorithms\u00b6","text":"<p>The challenge with knowing what good looks like is that it may be impractical to know the healthy thresholds for every metric in your system. A Relational Database Management System(RDBMS) can emit dozens of performance metrics, and when coupled with a microservices architecture you can potentially have hundreds of metrics that can impact your KPIs.</p> <p>Watching such a large number of datapoints and individually identifying their upper and lower thresholds may not always be practical for humans to do. But machine learning is very good at this sort of repetitive task. Leverage automation and machine learning wherever possible as it can help identify issues that you would otherwise not even know about!</p> <p>Success</p> <p>Use machine learning algorithms and anomaly detection models to automatically calculate your workload's performance thresholds.</p>"}]}